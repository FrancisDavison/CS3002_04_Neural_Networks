#Present inputs and outputs as sequence
#Set up input sequence
testinseq=trainin
predict_testNN=compute(NN,testinseq)
predict_out=as.numeric(predict_testNN$net.result>0.5)
predict_out
plot(NN)
#Assessed excercise answer:
#If the hidden layers is left at 0, the network will not be able to learn the weights correctly, however it learns the weights with the lowest error at hidden layers (3,2)
#CS3002_04_Neural_Networks_XOR
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#install.packages("neuralnet")
library(neuralnet)
#XOR gate input/output data
XORdat=cbind((trainout=rbind(0,1,1,0)),(trainin=rbind(c(1,1),c(1,-1),c(-1,1),c(-1.-1))))
#fit neural network with no hidden layers
set.seed(2) #Not sure what this does
NN=neuralnet(XORdat[,1]~., XORdat[,-1], hidden=c(3,3), threshold=0.001, stepmax=1e+05,linear.output=FALSE, rep=2)
#hidden layers error lowest at c(3,2)
#Threshold is the percepteron activation value
#Stepmax is the maximum number of steps for training
#linear output FALSE=Classification method, TRUE=Regression method (more accurate)
#Present inputs and outputs as sequence
#Set up input sequence
testinseq=trainin
predict_testNN=compute(NN,testinseq)
predict_out=as.numeric(predict_testNN$net.result>0.5)
predict_out
plot(NN)
#Assessed excercise answer:
#If the hidden layers is left at 0, the network will not be able to learn the weights correctly, however it learns the weights with the lowest error at hidden layers (3,2)
#CS3002_04_Neural_Networks_XOR
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#install.packages("neuralnet")
library(neuralnet)
#XOR gate input/output data
XORdat=cbind((trainout=rbind(0,1,1,0)),(trainin=rbind(c(1,1),c(1,-1),c(-1,1),c(-1.-1))))
#fit neural network with no hidden layers
set.seed(2) #Not sure what this does
NN=neuralnet(XORdat[,1]~., XORdat[,-1], hidden=c(3,3), threshold=0.001, stepmax=1e+05,linear.output=FALSE, rep=4)
#hidden layers error lowest at c(3,2)
#Threshold is the percepteron activation value
#Stepmax is the maximum number of steps for training
#linear output FALSE=Classification method, TRUE=Regression method (more accurate)
#Present inputs and outputs as sequence
#Set up input sequence
testinseq=trainin
predict_testNN=compute(NN,testinseq)
predict_out=as.numeric(predict_testNN$net.result>0.5)
predict_out
plot(NN)
#Assessed excercise answer:
#If the hidden layers is left at 0, the network will not be able to learn the weights correctly, however it learns the weights with the lowest error at hidden layers (3,2)
#CS3002_04_Neural_Networks_XOR
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#install.packages("neuralnet")
library(neuralnet)
#XOR gate input/output data
XORdat=cbind((trainout=rbind(0,1,1,0)),(trainin=rbind(c(1,1),c(1,-1),c(-1,1),c(-1.-1))))
#fit neural network with no hidden layers
set.seed(2) #Not sure what this does
NN=neuralnet(XORdat[,1]~., XORdat[,-1], hidden=c(3,3), threshold=0.001, stepmax=1e+05,linear.output=FALSE, rep=2)
#hidden layers error lowest at c(3,2)
#Threshold is the percepteron activation value
#Stepmax is the maximum number of steps for training
#linear output FALSE=Classification method, TRUE=Regression method (more accurate)
#Present inputs and outputs as sequence
#Set up input sequence
testinseq=trainin
predict_testNN=compute(NN,testinseq)
predict_out=as.numeric(predict_testNN$net.result>0.5)
predict_out
plot(NN)
#Assessed excercise answer:
#If the hidden layers is left at 0, the network will not be able to learn the weights correctly, however it learns the weights with the lowest error at hidden layers (3,2)
#CS3002_04_Neural_Networks_XOR
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#install.packages("neuralnet")
library(neuralnet)
#XOR gate input/output data
XORdat=cbind((trainout=rbind(0,1,1,0)),(trainin=rbind(c(1,1),c(1,-1),c(-1,1),c(-1.-1))))
#fit neural network with no hidden layers
set.seed(2) #Not sure what this does
NN=neuralnet(XORdat[,1]~., XORdat[,-1], hidden=c(3,2), threshold=0.001, stepmax=1e+05,linear.output=FALSE, rep=2)
#Changing rep(number of epochs) from 1 to 2 decreases error, but use any more and the error increases again
#hidden layers error lowest at c(3,2)
#Threshold is the percepteron activation value
#Stepmax is the maximum number of steps for training
#linear output FALSE=Classification method, TRUE=Regression method (more accurate)
#Present inputs and outputs as sequence
#Set up input sequence
testinseq=trainin
predict_testNN=compute(NN,testinseq)
predict_out=as.numeric(predict_testNN$net.result>0.5)
predict_out
plot(NN)
#Assessed excercise answer:
#If the hidden layers is left at 0, the network will not be able to learn the weights correctly, however it learns the weights with the lowest error at hidden layers (3,2)
#CS3002_04_Neural_Networks_XOR
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#install.packages("neuralnet")
library(neuralnet)
#XOR gate input/output data
XORdat=cbind((trainout=rbind(0,1,1,0)),(trainin=rbind(c(1,1),c(1,-1),c(-1,1),c(-1.-1))))
#fit neural network with no hidden layers
set.seed(2) #Not sure what this does
NN=neuralnet(XORdat[,1]~., XORdat[,-1], hidden=c(3,3), threshold=0.001, stepmax=1e+05,linear.output=FALSE, rep=2)
#Changing rep(number of epochs) from 1 to 2 decreases error, but use any more and the error increases again
#hidden layers error lowest at c(3,2)
#Threshold is the percepteron activation value
#Stepmax is the maximum number of steps for training
#linear output FALSE=Classification method, TRUE=Regression method (more accurate)
#Present inputs and outputs as sequence
#Set up input sequence
testinseq=trainin
predict_testNN=compute(NN,testinseq)
predict_out=as.numeric(predict_testNN$net.result>0.5)
predict_out
plot(NN)
#Assessed excercise answer:
#If the hidden layers is left at 0, the network will not be able to learn the weights correctly, however it learns the weights with the lowest error at hidden layers (3,2)
#CS3002_04_Neural_networks_Wine_Net
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#import and setup data
WineData=read.csv(".\\WineDataTrim.csv") #Uses trimmed csv containing only needed values. Crazy unpredictable and un fixable errors occur when trying to select columns from Winedata2
WineClass=WineData[,1]-1 #Select first column and subtract 1 from each value to make values in range 0-1 not 1-2
WineValue=WineData[,-1] #Select everything except forst column as WineVale
#Noramlise the values in WineDataTrim
WineValueNorm <- (WineValue-min(WineValue))/(max(WineValue)-min(WineValue)) #Normalise values (See notebook for explanation)
#set up test and  training sets
WineDataNorm <- (cbind(WineClass, WineValueNorm)) #cbind Class and Values back to one dataframe, then randomise with sample, then select rows 1:73 for training
WineDataNormRand=WineDataNorm[sample(130,130),]
WineDataTrain=WineDataNormRand[1:65,]
WineDataTest=WineDataNormRand[66:130,]
library(neuralnet) #imports neuralnet library
#set up neural nets
set.seed(2)
NN=neuralnet(WineDataTrain[,1]~., WineDataTrain[,-1], hidden=c(3,4,3),threshold=0.001,stepmax=1e+05,linear.output=FALSE)
#accuracy highest and error GENERALLY lowest with hidden layers 3,4,3
#I say generally because it will swinng wildly from 0.0009 to 1.5 for no apparent reason. Yay randomisation
predict_testNN=compute(NN,WineDataTest)
predict_out=as.numeric(predict_testNN$net.result>0.5)
ncorrect=sum(predict_out==WineDataTest[,1])
n=length(WineDataTest[,1])
accuracy=ncorrect/n
print(accuracy)
plot(NN)
#CS3002_04_Neural_networks_Wine_Net
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#import and setup data
WineData=read.csv(".\\WineDataTrim.csv") #Uses trimmed csv containing only needed values. Crazy unpredictable and un fixable errors occur when trying to select columns from Winedata2
WineClass=WineData[,1]-1 #Select first column and subtract 1 from each value to make values in range 0-1 not 1-2
WineValue=WineData[,-1] #Select everything except forst column as WineVale
#Noramlise the values in WineDataTrim
WineValueNorm <- (WineValue-min(WineValue))/(max(WineValue)-min(WineValue)) #Normalise values (See notebook for explanation)
#set up test and  training sets
WineDataNorm <- (cbind(WineClass, WineValueNorm)) #cbind Class and Values back to one dataframe, then randomise with sample, then select rows 1:73 for training
WineDataNormRand=WineDataNorm[sample(130,130),]
WineDataTrain=WineDataNormRand[1:65,]
WineDataTest=WineDataNormRand[66:130,]
library(neuralnet) #imports neuralnet library
#set up neural nets
set.seed(2)
NN=neuralnet(WineDataTrain[,1]~., WineDataTrain[,-1], hidden=c(3,4,3),threshold=0.001,stepmax=1e+05,linear.output=FALSE, rep=2)
#accuracy highest and error GENERALLY lowest with hidden layers 3,4,3
#I say generally because it will swinng wildly from 0.0009 to 1.5 for no apparent reason. Yay randomisation
predict_testNN=compute(NN,WineDataTest)
predict_out=as.numeric(predict_testNN$net.result>0.5)
ncorrect=sum(predict_out==WineDataTest[,1])
n=length(WineDataTest[,1])
accuracy=ncorrect/n
print(accuracy)
plot(NN)
#CS3002_04_Neural_networks_Wine_Net
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#import and setup data
WineData=read.csv(".\\WineDataTrim.csv") #Uses trimmed csv containing only needed values. Crazy unpredictable and un fixable errors occur when trying to select columns from Winedata2
WineClass=WineData[,1]-1 #Select first column and subtract 1 from each value to make values in range 0-1 not 1-2
WineValue=WineData[,-1] #Select everything except forst column as WineVale
#Noramlise the values in WineDataTrim
WineValueNorm <- (WineValue-min(WineValue))/(max(WineValue)-min(WineValue)) #Normalise values (See notebook for explanation)
#set up test and  training sets
WineDataNorm <- (cbind(WineClass, WineValueNorm)) #cbind Class and Values back to one dataframe, then randomise with sample, then select rows 1:73 for training
WineDataNormRand=WineDataNorm[sample(130,130),]
WineDataTrain=WineDataNormRand[1:65,]
WineDataTest=WineDataNormRand[66:130,]
library(neuralnet) #imports neuralnet library
#set up neural nets
set.seed(2)
NN=neuralnet(WineDataTrain[,1]~., WineDataTrain[,-1], hidden=c(3,4,3),threshold=0.001,stepmax=1e+05,linear.output=FALSE, rep=3)
#accuracy highest and error GENERALLY lowest with hidden layers 3,4,3
#I say generally because it will swinng wildly from 0.0009 to 1.5 for no apparent reason. Yay randomisation
predict_testNN=compute(NN,WineDataTest)
predict_out=as.numeric(predict_testNN$net.result>0.5)
ncorrect=sum(predict_out==WineDataTest[,1])
n=length(WineDataTest[,1])
accuracy=ncorrect/n
print(accuracy)
plot(NN)
#CS3002_04_Neural_networks_Wine_Net
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#import and setup data
WineData=read.csv(".\\WineDataTrim.csv") #Uses trimmed csv containing only needed values. Crazy unpredictable and un fixable errors occur when trying to select columns from Winedata2
WineClass=WineData[,1]-1 #Select first column and subtract 1 from each value to make values in range 0-1 not 1-2
WineValue=WineData[,-1] #Select everything except forst column as WineVale
#Noramlise the values in WineDataTrim
WineValueNorm <- (WineValue-min(WineValue))/(max(WineValue)-min(WineValue)) #Normalise values (See notebook for explanation)
#set up test and  training sets
WineDataNorm <- (cbind(WineClass, WineValueNorm)) #cbind Class and Values back to one dataframe, then randomise with sample, then select rows 1:73 for training
WineDataNormRand=WineDataNorm[sample(130,130),]
WineDataTrain=WineDataNormRand[1:65,]
WineDataTest=WineDataNormRand[66:130,]
library(neuralnet) #imports neuralnet library
#set up neural nets
set.seed(2)
NN=neuralnet(WineDataTrain[,1]~., WineDataTrain[,-1], hidden=c(3,4,3),threshold=0.001,stepmax=1e+05,linear.output=FALSE, rep=3)
#accuracy highest and error GENERALLY lowest with hidden layers 3,4,3
#I say generally because it will swinng wildly from 0.0009 to 1.5 for no apparent reason. Yay randomisation
predict_testNN=compute(NN,WineDataTest)
predict_out=as.numeric(predict_testNN$net.result>0.5)
ncorrect=sum(predict_out==WineDataTest[,1])
n=length(WineDataTest[,1])
accuracy=ncorrect/n
print(accuracy)
plot(NN)
#CS3002_04_Neural_networks_Wine_Net
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#import and setup data
WineData=read.csv(".\\WineDataTrim.csv") #Uses trimmed csv containing only needed values. Crazy unpredictable and un fixable errors occur when trying to select columns from Winedata2
WineClass=WineData[,1]-1 #Select first column and subtract 1 from each value to make values in range 0-1 not 1-2
WineValue=WineData[,-1] #Select everything except forst column as WineVale
#Noramlise the values in WineDataTrim
WineValueNorm <- (WineValue-min(WineValue))/(max(WineValue)-min(WineValue)) #Normalise values (See notebook for explanation)
#set up test and  training sets
WineDataNorm <- (cbind(WineClass, WineValueNorm)) #cbind Class and Values back to one dataframe, then randomise with sample, then select rows 1:73 for training
WineDataNormRand=WineDataNorm[sample(130,130),]
WineDataTrain=WineDataNormRand[1:65,]
WineDataTest=WineDataNormRand[66:130,]
library(neuralnet) #imports neuralnet library
#set up neural nets
set.seed(2)
NN=neuralnet(WineDataTrain[,1]~., WineDataTrain[,-1], hidden=c(3,4,3),threshold=0.001,stepmax=1e+05,linear.output=FALSE, rep=3)
#accuracy highest and error GENERALLY lowest with hidden layers 3,4,3
#Increasing rep(epochs) will decrease the error and increase the accuracy
predict_testNN=compute(NN,WineDataTest)
predict_out=as.numeric(predict_testNN$net.result>0.5)
ncorrect=sum(predict_out==WineDataTest[,1])
n=length(WineDataTest[,1])
accuracy=ncorrect/n
print(accuracy)
plot(NN)
#CS3002_04_Neural_networks_Wine_Net
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#import and setup data
WineData=read.csv(".\\WineDataTrim.csv") #Uses trimmed csv containing only needed values. Crazy unpredictable and un fixable errors occur when trying to select columns from Winedata2
WineClass=WineData[,1]-1 #Select first column and subtract 1 from each value to make values in range 0-1 not 1-2
WineValue=WineData[,-1] #Select everything except forst column as WineVale
#Noramlise the values in WineDataTrim
WineValueNorm <- (WineValue-min(WineValue))/(max(WineValue)-min(WineValue)) #Normalise values (See notebook for explanation)
#set up test and  training sets
WineDataNorm <- (cbind(WineClass, WineValueNorm)) #cbind Class and Values back to one dataframe, then randomise with sample, then select rows 1:73 for training
WineDataNormRand=WineDataNorm[sample(130,130),]
WineDataTrain=WineDataNormRand[1:65,]
WineDataTest=WineDataNormRand[66:130,]
library(neuralnet) #imports neuralnet library
#set up neural nets
set.seed(2)
NN=neuralnet(WineDataTrain[,1]~., WineDataTrain[,-1], hidden=c(3,4,3),threshold=0.001,stepmax=1e+05,linear.output=FALSE, rep=3)
#accuracy highest and error GENERALLY lowest with hidden layers 3,4,3
#Increasing rep(epochs) will decrease the error and increase the accuracy
predict_testNN=compute(NN,WineDataTest)
predict_out=as.numeric(predict_testNN$net.result>0.5)
ncorrect=sum(predict_out==WineDataTest[,1])
n=length(WineDataTest[,1])
accuracy=ncorrect/n
print(accuracy)
plot(NN)
#CS3002_04_Neural_networks_Wine_Net
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#import and setup data
WineData=read.csv(".\\WineDataTrim.csv") #Uses trimmed csv containing only needed values. Crazy unpredictable and un fixable errors occur when trying to select columns from Winedata2
WineClass=WineData[,1]-1 #Select first column and subtract 1 from each value to make values in range 0-1 not 1-2
WineValue=WineData[,-1] #Select everything except forst column as WineVale
#Noramlise the values in WineDataTrim
WineValueNorm <- (WineValue-min(WineValue))/(max(WineValue)-min(WineValue)) #Normalise values (See notebook for explanation)
#set up test and  training sets
WineDataNorm <- (cbind(WineClass, WineValueNorm)) #cbind Class and Values back to one dataframe, then randomise with sample, then select rows 1:73 for training
WineDataNormRand=WineDataNorm[sample(130,130),]
WineDataTrain=WineDataNormRand[1:65,]
WineDataTest=WineDataNormRand[66:130,]
library(neuralnet) #imports neuralnet library
#set up neural nets
set.seed(2)
NN=neuralnet(WineDataTrain[,1]~., WineDataTrain[,-1], hidden=c(3,4,3),threshold=0.001,stepmax=1e+05,linear.output=FALSE, rep=3)
#accuracy highest and error GENERALLY lowest with hidden layers 3,4,3
#Increasing rep(epochs) will decrease the error and increase the accuracy
predict_testNN=compute(NN,WineDataTest)
predict_out=as.numeric(predict_testNN$net.result>0.5)
ncorrect=sum(predict_out==WineDataTest[,1])
n=length(WineDataTest[,1])
accuracy=ncorrect/n
print(accuracy)
plot(NN)
#CS3002_04_Neural_networks_Wine_Net
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#import and setup data
WineData=read.csv(".\\WineDataTrim.csv") #Uses trimmed csv containing only needed values. Crazy unpredictable and un fixable errors occur when trying to select columns from Winedata2
WineClass=WineData[,1]-1 #Select first column and subtract 1 from each value to make values in range 0-1 not 1-2
WineValue=WineData[,-1] #Select everything except forst column as WineVale
#Noramlise the values in WineDataTrim
WineValueNorm <- (WineValue-min(WineValue))/(max(WineValue)-min(WineValue)) #Normalise values (See notebook for explanation)
#set up test and  training sets
WineDataNorm <- (cbind(WineClass, WineValueNorm)) #cbind Class and Values back to one dataframe, then randomise with sample, then select rows 1:73 for training
WineDataNormRand=WineDataNorm[sample(130,130),]
WineDataTrain=WineDataNormRand[1:65,]
WineDataTest=WineDataNormRand[66:130,]
library(neuralnet) #imports neuralnet library
#set up neural nets
set.seed(2)
NN=neuralnet(WineDataTrain[,1]~., WineDataTrain[,-1], hidden=c(3,4,3),threshold=0.001,stepmax=1e+05,linear.output=FALSE, rep=3)
#accuracy highest and error GENERALLY lowest with hidden layers 3,4,3
#Increasing rep(epochs) will decrease the error and increase the accuracy
predict_testNN=compute(NN,WineDataTest)
predict_out=as.numeric(predict_testNN$net.result>0.5)
ncorrect=sum(predict_out==WineDataTest[,1])
n=length(WineDataTest[,1])
accuracy=ncorrect/n
print(accuracy)
plot(NN)
#CS3002_04_Neural_Networks_XOR
#Sets workiing Directory based on if it's being run on laptop or desktop
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#install.packages("neuralnet")
library(neuralnet)
#XOR gate input/output data
XORdat=cbind((trainout=rbind(0,1,1,0)),(trainin=rbind(c(1,1),c(1,-1),c(-1,1),c(-1.-1))))
#fit neural network with no hidden layers
set.seed(2) #Not sure what this does
NN=neuralnet(XORdat[,1]~., XORdat[,-1], hidden=c(3,3), threshold=0.001, stepmax=1e+05, linear.output=FALSE, rep=2)
#Changing rep(number of epochs) from 1 to 2 decreases error, but use any more and the error increases again
#hidden layers error lowest at c(3,2)
#Threshold is the percepteron activation value
#Stepmax is the maximum number of steps for training
#linear output FALSE=Classification method, TRUE=Regression method (more accurate)
#Present inputs and outputs as sequence
#Set up input sequence
testinseq=trainin
predict_testNN=compute(NN,testinseq)
predict_out=as.numeric(predict_testNN$net.result>0.5)
predict_out
plot(NN)
#Assessed excercise answer:
#If the hidden layers is left at 0, the network will not be able to learn the weights correctly, however it learns the weights with the lowest error at hidden layers (3,2)
#CS3002_04_Neural_Networks_XOR
#Sets workiing Directory based on if it's being run on laptop or desktop
if((Sys.info()["nodename"])=="JANEWAY")
{
setwd("C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working Directory set to C:\\Users\\Janeway\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
if((Sys.info()["nodename"])=="PICARD")
{
setwd("C:\\Users\\Picard\\Dropbox\\Github\\CS3002_CS3002_04_Neural_Networks")
print("Working directory set to C:\\Users\\Picard\\Dropbox\\Github\\CS3002_04_Neural_Networks")
}
#install.packages("neuralnet")
library(neuralnet)
#XOR gate input/output data
XORdat=cbind((trainout=rbind(0,1,1,0)),(trainin=rbind(c(1,1),c(1,-1),c(-1,1),c(-1.-1))))
#fit neural network with no hidden layers
set.seed(2) #Not sure what this does
NN=neuralnet(XORdat[,1]~., XORdat[,-1], hidden=c(3,2), threshold=0.001, stepmax=1e+05, linear.output=FALSE, rep=2)
#Changing rep(number of epochs) from 1 to 2 decreases error, but use any more and the error increases again
#hidden layers error lowest at c(3,2)
#Threshold is the percepteron activation value
#Stepmax is the maximum number of steps for training
#linear output FALSE=Classification method, TRUE=Regression method (more accurate)
#Present inputs and outputs as sequence
#Set up input sequence
testinseq=trainin
predict_testNN=compute(NN,testinseq)
predict_out=as.numeric(predict_testNN$net.result>0.5)
predict_out
plot(NN)
#Assessed excercise answer:
#If the hidden layers is left at 0, the network will not be able to learn the weights correctly, however it learns the weights with the lowest error at hidden layers (3,2)
